<?xml version="1.0" encoding="UTF-8" ?>

<chapter acro="R" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns="">
<title>Representations</title>

<introduction>
<p>Previous work with linear transformations may have convinced you that we can convert most questions about linear transformations into questions about systems of equations or properties of subspaces of $\complex{m}$.  In this section we begin to make these vague notions precise.   We have used the word <q>representation</q> prior, but it will get a heavy workout in this chapter.  In many ways, everything we have studied so far was in preparation for this chapter.</p>

</introduction>

<xi:include href="./section-VR.xml" />
<xi:include href="./section-MR.xml" />
<xi:include href="./section-CB.xml" />
<xi:include href="./section-OD.xml" />
<annotatedacronyms>
<!-- %%%%%%%%%% -->
<!-- % -->
<!-- %  Annotated Acronyms R -->
<!-- %  Representations -->
<!-- % -->
<!-- %%%%%%%%%% -->
<annoacro type="definition" acro="VR">
<p>
Matrix representations build on vector representations, so this is the definition that gets us started.  A representation depends on the choice of a single basis for the vector space.  <acroref type="theorem" acro="VRRB" /> is what tells us this idea might be useful.
</p>
</annoacro>
<annoacro type="theorem" acro="VRILT">
<p>
As an invertible linear transformation, vector representation allows us to translate, back and forth, between abstract vector spaces ($V$) and concrete vector spaces ($\complex{n}$).  This is key to all our notions of representations in this chapter.
</p>
</annoacro>
<annoacro type="theorem" acro="CFDVS">
<p>
Every vector space with finite dimension <q>looks like</q> a vector space of column vectors.  Vector representation is the isomorphism that establishes that these vector spaces are isomorphic.
</p>
</annoacro>
<annoacro type="definition" acro="MR">
<p>
Building on the definition of a vector representation, we define a representation of a linear transformation, determined by a choice of two bases, one for the domain and one for the codomain.  Notice that vectors are represented by columnar lists of scalars, while linear transformations are represented by rectangular tables of scalars.  Building a matrix representation is as important a skill as row-reducing a matrix.
</p>
</annoacro>
<annoacro type="theorem" acro="FTMR">
<p>
<acroref type="definition" acro="MR" /> is not really very interesting until we have this theorem.  The second form tells us that we can compute outputs of linear transformations via matrix multiplication, along with some bookkeeping for vector representations.  Searching forward through the text on <q>FTMR</q> is an interesting exercise.  You will find reference to this result buried inside many key proofs at critical points, and it also appears in numerous examples and solutions to exercises.
</p>
</annoacro>
<annoacro type="theorem" acro="MRCLT">
<p>
Turns out that matrix multiplication is really a very natural operation, it is just the chaining together (composition) of functions (linear transformations).  Beautiful.  Even if you do not try to work the problem, study <acroref type="solution" acro="MR.T80" /> for more insight.
</p>
</annoacro>
<annoacro type="theorem" acro="KNSI">
<p>
Kernels <q>are</q> null spaces.  For this reason you will see these terms used interchangeably.
</p>
</annoacro>
<annoacro type="theorem" acro="RCSI">
<p>
Ranges <q>are</q> column spaces.  For this reason you will see these terms used interchangeably.
</p>
</annoacro>
<annoacro type="theorem" acro="IMR">
<p>
Invertible linear transformations are represented by invertible (nonsingular) matrices.
</p>
</annoacro>
<annoacro type="theorem" acro="NME9">
<p>
The NMEx series has always been important, but we have held off saying so until now.  This is the end of the line for this one, so it is a good time to contemplate all that it means.
</p>
</annoacro>
<annoacro type="theorem" acro="SCB">
<p>
Diagonalization back in <acroref type="section" acro="SD" /> was really a change of basis to achieve a diagonal matrix repesentation.  Maybe we should be highlighting the more general <acroref type="theorem" acro="MRCB" /> here, but its overly technical description just is not as appealing.  However, it will be important in some of the matrix decompositions you will encounter in a future course in linear algebra.
</p>
</annoacro>
<annoacro type="theorem" acro="EER">
<p>
This theorem, with the companion definition, <acroref type="definition" acro="EELT" />, tells us that eigenvalues, and eigenvectors, are fundamentally a characteristic of linear transformations (not matrices).  If you study matrix decompositions in a future course in linear algebra you will come to appreciate that almost all of a matrix's secrets can be unlocked with knowledge of the eigenvalues and eigenvectors.
</p>
</annoacro>
<annoacro type="theorem" acro="OD">
<p>
Can you imagine anything nicer than an orthonormal diagonalization?  A basis of pairwise orthogonal, unit norm, eigenvectors that provide a diagonal representation for a matrix?  Here we learn just when this can happen <mdash /> precisely when a matrix is normal, which is a disarmingly simple property to define.
</p>
</annoacro>
<!--  End R.tex annotated acronyms -->
</annotatedacronyms>
</chapter>